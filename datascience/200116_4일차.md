[toc]

# 200116_ 데이터사이언스_ 4일차



## Review

#### 파이썬 장점

dictionary 데이터 타입이 json 형식과 동일하다. 따라서, 웹 개발에 용이하다. (중소규모)

데이터분석 관점에서는 python이 수집 -> EDA(탐색) -> 가공 - > 분석 -> 머신러닝 -> 서비스 라는 일련의 전 과정을 다 수행할 수 있다. 반면에 R은 수집과 EDA밖에 못한다.

#### EDA

`value_counts`를 통해 데이터의 카테고리화 상황을 확인하고 feature로 사용할 수 있는지 생각해본다.



## Visualization

matplotlib의 일반 plots과 seaborn의 plots 를 사용할 수 있다.

시각화의 목적은 시각화 도구를 사용하여 데이터 탐색을 수행한다. 그리고 의미있는 결과를 도출하여 파생 데이터를 추가 할 수 있어야 한다.

- EX)

  아래 그래프에서 약 18달러를 기준으로 생존여부가 크게 달라지는 것을 확인할 수 있다. 따라서 18달러를 기준으로 고가와 저가 파생 컬럼을 생성할 수 있다.

  ![시각화로 파생 컬럼 도출하기](C:\git\TIL\datascience\images\시각화&파생컬럼생성.png)

  

#### seaborn 주요 plots

` sns.countplot(data=train, x='Pclass', hue='Survived')`

- y축이 count

`sns.barplot(data=train, x='Pclass', y='Fare', hue='Survived')`

- min, max, mean 등 데이터 분포 쉽게 파악 가능
- 카테고리화 데이터를 x축, 분포를 알고 싶은 데이터를 y축

`sns.pointplot(data=train, x='Pclass', y='Fare', hue='Survived')`

- barplot과 비슷, barplot은 카테고리 데이터(Pclass) 끼리는 관계가 없다고 판단

  반면, pointplot은 서로 관계있다고 판단

`sns.distplot(survived['Fare'], hist=False, label='survived')`

- 분포도

```python
figure, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3)
figure.set_size_inches(15, 4)

sns.countplot(data=train, x='Pclass', hue='Survived', ax=ax1)
sns.countplot(data=train, x='Sex', hue='Survived', ax=ax2)
sns.countplot(data=train, x='Embarked', hue='Survived', ax=ax3)
```

- 다수의 plots 출력



## 머신러닝

#### 분류

- 지도학습 : 데이터와 결과 값을 주고 학습

  - Classification : [분류모델], 대출상담 가능 여부와 같이 두 가지 결과만을 만드는 것은 바이너리 classification, 이미처리에서 0~9의 숫자로 손글씨 인식(mnist) 하는 방식은 멀티 레벨 classfication라고 한다.
  - Regression : [예측모델]숫자 자체를 예측한다. 예를 들면, 공장의 컨티션을 입력하여 재고량을 예측하는 것

- 비지도학습 : 입력 데이터만 준다. 컬럼 수에 맞는 차원의 벡터를 만들어 거리를 계산한다. 이를 통해 입력데이터를 군집화한다. (클러스터링)

  EX) 고객 특성 분류 / 공부할 때 듣기 좋은 음악 분류 등...

- 강화학습 : 반 비지도 학습, 인간이 학습하는 방법과 유사하다. 상호작용(피드백) 속에서 모델을 강화해 나간다. 

  EX) 알파고제로 : 기존 데이터가 없는 상태에서 입력 값을 주고 결과가 YES/NO라는 피드백만 줘서 모델을 강화해나간다.

#### vs 딥러닝



#### Scikit-lean

지도학습과 비지도학습 머신러닝 라이브러리로 딥러닝의 tensorflow 같은 역할을 한다.



#### 머신러닝 성능을 높이는 3가지 방법

1. Feature Engineering

   - 시각화 도구를 활용하여 의미 파악 후 feautre에 포함시킬지 결정한다.
   - cross validation

2. 모델 선택 : 어떤 알고리즘을 선택하는가

3. 매개변수 조정(Hyper parameter 튜닝) : 모델에 참여하는 트리의 개수 등...

   - EX) 그리드 방식으로 매개변수를 조정한다.

     max_depth / n_estimators를 for문을 이용하여 일일이 수정



#### 머신러닝 흐름

![Machine Leaning flow](C:\git\TIL\datascience\images\머신런닝흐름.PNG)

데이터 학습 단계 전 데이터 분리가 되어야 한다. Train 데이터와 Test 데이터로 나눈다.

3-1 단계에서 알고리즘을 선택한다. 분류와 예측 모델에 있어 주로 Random Forest를 사용한다. 

3-2 단계에서 H.P Tuning한다. 



테스트 데이터와 훈련 데이터 구분 간 훈련 데이터가 특정 결과 값에 편향 되어있을 수가 있다. 이럴 경우 overfitting이 발생할 수 있다. 이를 막고자 Cross Validation 이라는 방법을 사용한다. 이는 데이터 구분을 각기 달리하여 반복적으로 훈련시키는 것을 의미한다.

>overfitting : 너무 훈련데이터에 맞춰져있어 모델의 일반화하기가 어려운 상태

## 

## Kaggle

getting started competition 에서 연습하기

Notebooks 에서 공부하기

#### titanic



#### bike sharing

windspeed = 0(누락데이터) 찾기

H.P 튜닝

datetime - objects 가공 필요

randomfoestregressor

- n_estimators : tree 몇개 돌릴것인가?

kaggle's regression scoring 원리 : 결과값과 정답의 차이들의 합(RootMeanSquare 방식(RMSE))

RMSLE 방식은 기존 제곱 값에 루트+1한 값이다. 에러 값이 많이 나도 그 합의 값을 줄이기 위해 사용



## 명령어 & 사용 함수

`%matplotlib inine`

jupyter notebook에서 사용하는 명령어. inline을 통해 화면에 출력을 위한 명령어

`parse_dates = ["datetime"]`

파이썬 내부에서 날짜를 다 나눔, 만년월도 가지고 있어 해당일의 요일까지 출력 가능하다.

## 참고

- [데이터 시각화 도구인 seaborn 튜토리얼 및 API] http://seaborn.pydata.org/
